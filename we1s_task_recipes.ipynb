{
 "metadata": {
  "name": "",
  "signature": "sha256:a0d64194b144d47a82199ab024fb19bb35b858a0811503bc349f51914918b7e6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Starting an iPython Notebook:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Open Canopy\n",
      "* Tools > Canopy > Canopy Command Prompt\n",
      "* Then: \"ipython notebook\" will open the iPython notebook environment\n",
      "* Then \"create new notebook\""
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "WhatEvery1Says Project Task Recipes:"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "C. Using the \"get-nytimes-articles.py\" script"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* This Python script executes a query on the NYT articles API and automatically processes/aggregates the JSON-format results in a TSV (tab separated values) file, ready for import into Excel or another spreadsheet program.  The resulting sheet has columns for all the returned results--including URL, byline, type of document, section of NYT, etc.\n",
      "* By default, the Python script is designed to submit a simple \"q\" query for a keyword or phrase.  One sets the query in the separate plain-text \"settings.cfg\" file (in the config subfolder of the script).  For example, to search for \"apples, oranges, cherries,\" you would set the query=apples+oranges+cherries.\n",
      "* In order to use the Python script to submit to the NYT a \"fq\" query and other modifications allowing for finer adjustments of the sort we will be interested in for WE1S, one needs to tweak both the query= line in the \"settings.cfg\" file and the \"request_string\" line in the main Python script (so as to generate the total request string that one wants).\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* Github source for get-nytimes-articles.py script (and an example settings.cfg file) -- Use \"download zip\" at bottom of right sidebar on the Github web page to download the whole package: https://github.com/casmlab/get-nytimes-articles\n",
      "* Note 1: In the settings.cfg file or the script, you need to define a folder on your computer for storing the JSON files that a successful query to the NYT generates (e.g., json_folder = C:/workspace/).  You also need to define the path of the TSV file in which  the script aggregates the individual JSON results (e.g., tsv_file = C:/workspace/output.tsv).  In addition, you need to input your NYT API key.\n",
      "* Note 2: If you run the script and the query is valid, then your Python window will appear to freeze for some time (a really long time if you are searching through a large span of dates) before you are returned to a command prompt.  Check your defined workspace for the JSON and TSV files to see if the script is working properly.  You'll see one JSON file after another added; and the TSV file is added to in each instance.  If your query is invalid, you'll either get an \"invalid syntax\" error message from Python or a running series of \"error . . .  bad request\" messages that have to be stopped by interrupting or restarting the Python kernel.\n",
      "* Note 3: If you are just experimenting, set the begin and end date parameters of a search to a short period (e.g., three months or less).  To give you a sense of time scale: when I searched for \"humanities\" over three years of the NYT, it took about an hour.  Doing the same search for three months makes it a reasonable 5-10 minutes.  (You can check individual JSON files as they are added at any time to see what is happening if you want, even before the whole search is completed.  So you don't have to wait for a completed search.)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Recipes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Use the get-nytmes-articles.py script to do a proximity search for the words \"humanities\" and \"arts\" within 3 words of each other:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. In the settings.cfg file, set the query line as follows (using Lucene query syntax):\n",
      "query = \"humanities+arts\"~3\n",
      "2. In the Python script, set the request string to use \"fq\" instead of just \"q.\"  So the line would begin:\n",
      "request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?fq="
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Use the get-nytmes-articles.py script to do a proximity search and also limit the search to \"The New York Times\" as the \"source\" field (versus all the other sources that the NYT carries, including AP, International Herald Tribune, etc.):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "1. In the settings.cfg file, set the query line as follows (using Lucene query syntax): <br />query = \"humanities+arts\"~3\n",
      "2. In Python script:<br />request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=source%3A%28%22The+New+York+Times%22%29+AND+\" + query + \"&begin_date=\" + date + \"&end_date=\" + date + \"&page=\" + str(page) + \"&api-key=\" + api_key\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Use the get-nytimes-articles.py script to do a proximity search, limit the search to \"The New York Times\" as the \"source\" field, and exclude selected \"news_desks\":"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. In the settings.cfg file, set the query line as follows (using Lucene query syntax):\n",
      "query = \"humanities+arts\"~3\n",
      "2. In the Python script, set the request string to use \"fq\" instead of just \"q.\"  So the line would begin:\n",
      "request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=source%3A%28%22The+New+York+Times%22%29+NOT+news_desk.contains%3A%28%22Sports%22+%22Classified%22%29+AND+\" + query + \"&begin_date=\" + date + \"&end_date=\" + date + \"&page=\" + str(page) + \"&api-key=\" + api_key"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "D. Downloading web pages from list of URLs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. DownloadThemAll addon for Firefox, https://addons.mozilla.org/en-US/firefox/addon/downthemall/\n",
      "2. Note: This seems the simplest solution.  Wget for various OS platforms is also used for this purpose."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "E. Scraping plain text from web pages or PDFs"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Recipes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the below recipies, \"testfile,\" \"raw,\" etc. are arbitrarily named variables.<br /> For file paths, etc., substitute your own."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Read local HTML file into Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "print testfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Scrape plain text from a local Web page using Python and BeautifulSoup 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "print raw "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Scrape all plain text from an online Web page using Python and BeautifulSoup 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "import re\n",
      "\n",
      "url=\"http://www.english.ucsb.edu/\"\n",
      "page=urllib2.urlopen(url)\n",
      "soup = BeautifulSoup(page.read())\n",
      "print(soup.get_text())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4.Scrape plain text only from inside the &lt; p class=\"story-body-text story-content\"&gt; tags in NYT article using Python and BeautifulSoup 4 -- <br />Method: findall relevant p tags; then loop through the tags and print only the text they contain"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "soup = BeautifulSoup(testfile)\n",
      "\n",
      "soup.find_all('p',{'class':'story-body-text story-content'})\n",
      "\n",
      "for p_tag in soup.find_all('p'):\n",
      "    print p_tag.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "By REUTERSFEB. 8, 2014, 5:31 P.M. E.S.T.\n",
        "\n",
        "MINNEAPOLIS \u2014  Family and friends of Joan Mondale, the wife of former U.S. Vice President Walter Mondale, gathered on Saturday for a memorial service to celebrate the life of the woman known as \"Joan of Art\" for her decades of support for the fine arts.\n",
        "She died on Monday at age 83.\n",
        "Besides Walter Mondale, former President Jimmy Carter, U.S. Vice President Joe Biden and Minnesota Governor Mark Dayton were among the dignitaries at the downtown Minneapolis church where the service was held for the former second lady of the 1977-1981 Carter administration.\n",
        "In tributes filled with music and humor, speakers described Joan Mondale as independent-minded and caring.\n",
        "\"Joan Mondale was a treasure, and like all great treasures her memory will be as powerful as her presence on those who loved her and those of you she loved so much,\" Biden told the congregation.\n",
        "Her support for the arts spanned more than six decades from her college studies and work, through her efforts to promote artists and arts programs during and after Walter Mondale's terms as a U.S. senator, vice president and ambassador.\n",
        "After moving to Washington when her husband became a U.S. senator in 1964, Joan Mondale devoted much of her time to the National Gallery of Art. Her book, \"Politics in Art,\" is based on her arts lectures.\n",
        "Joan Mondale filled the vice president's residence with contemporary American art. She served as honorary chair of the Federal Council on Arts and Humanities for the Carter Administration, advocating for private and public arts support across the country.\n",
        "DIARY ENTRY\n",
        "Former President Carter recalled that Joan Mondale persistently coaxed him to foster the arts during his administration.\n",
        "He drew laughter when he read a diary entry from 1978, the year he oversaw the historic Camp David peace accords between Israel and Egypt. It said that the time he spent choosing members and directors of the National Endowment for the Arts and the National Endowment for the Humanities exceeded the time he \"spent bringing peace to the Middle East.\"\n",
        "Carter said Joan Mondale left a lasting effect on his administration and the work of Walter Mondale, whom he and other speakers referred to by his nickname \"Fritz.\"\n",
        "\"We accommodated Joan's dreams and I would say it was one of the most substantive and long-lasting impacts on the national scene that I left behind me and Fritz left behind him,\" he said.\n",
        "Joan Mondale brought American art to the embassy and immersed herself in Japanese art when her husband served as President Bill Clinton's ambassador to Japan. She incorporated Japanese styles into her pottery and visited Japanese artisans.\n",
        "Masaharu Yoshida, consul general of Japan at Chicago, also attended the service.\n",
        "Biden said in his years of knowing Joan Mondale, he remembers that she pushed as hard for social causes as she did to promote the arts, and he mentioned her support of campaigns for equal pay for women.\n",
        "Joan Mondale served on the board of the Minnesota Orchestra and the Walker Art Center in Minneapolis and other boards and as a trustee at Macalester College, her alma mater and the school where her father served as chaplain.\n",
        "The service was held at Westminster Presbyterian Church where the Mondales were members. Bagpipers and drummers led the congregants to a reception at nearby Orchestra Hall.\n",
        "The Mondales were married 58 years, meeting and marrying in 1955. They had three children: Eleanor, Ted and William. Eleanor Mondale died in 2011 from brain cancer.\n",
        "(Reporting by David Bailey in Minneapolis, additional reporting by Alex Dobuzinskis in Los Angeles, Editing by Lisa Shumaker and Gunna Dickson)\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "F. Using Python NLTK package (with BeautifulSoup 4) to do text manipulation and analysis"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Recipes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the below recipies, \"testfile,\" \"raw,\" etc. are arbitrarily named variables.<br /> For file paths, etc., substitute your own."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK, then tokenize and inspect tokens using NLTK"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "number_of_tokens= len(tokens)\n",
      "s = \"Total number of tokens = \"\n",
      "print s+str(number_of_tokens)\n",
      "\n",
      "first_ten_tokens = tokens[:10]\n",
      "s2 = \"First 10 tokens =\"\n",
      "print s2+str(first_ten_tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">concordance (for a target word)</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.concordance(\"humanities\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">collocates in document</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.collocations()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">words used in similar context as target word</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.similar(\"art\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "5. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">common contexts of 2 or more target words</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.common_contexts([\"business\", \"arts\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}