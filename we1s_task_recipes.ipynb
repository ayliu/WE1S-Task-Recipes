{
 "metadata": {
  "name": "",
  "signature": "sha256:ebdc35ce7979e71db793c0042a02f5a8849542046a5416d8727e0218b6addcd0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# <a name=\"top\"></a>Table of Contents\n",
      "###* <a href=\"#startingipynb\">Starting an iPython Notebook</a>\n",
      "###* <a href=\"#recipes\">WhatEvery1Says Task Recipes</a>\n",
      "  * <a href=\"#recipes-getnyt\">C. Using the \"get-nytimes-articles.py\" script</a>\n",
      "  * <a href=\"#recipes-downloading\">D. Downloading web pages from list of URLs</a>\n",
      "  * <a href=\"#recipes-scraping\">E. Scraping plain text from web pages or PDFs</a>\n",
      "  * <a href=\"#recipes-nltk\">F. Using Python NLTK package (with BeautifulSoup 4) to do text manipulation and analysis</a>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<div id=\"startingipynb\" style=\"padding: 1em; background-color: #cccccc;\">Starting an iPython Notebook:</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "*The following assumes that you have iPython Notebook available as part of a Python environment on your computer--e.g., as part of the Enthought Canopy distribution of Python.  (The examples below assume you have Canopy python installed.)*\n",
      "\n",
      "####Method 1\n",
      "* Navigate to folder where you store or have downloaded the .ipynb iPython Notebook file.\n",
      "* Click on the file, which should open in the top pane (file pane) of the Canopy python integrated development environment.\n",
      "* Right click in the pane, and choose the menu item: \"open in web browser.\"\n",
      "* The iPython Notebook will open in your browser.\n",
      "\n",
      "####Method 2\n",
      "* Open Canopy (but not the Canopy Editor)\n",
      "* Go to Tools > Canopy > Canopy Command Prompt\n",
      "* When the command window opens, navigate using \"cd\" (change directory) commands to the folder where you store or have downloaded the .ipynb iPython Notebook file.\n",
      "* While stille in the command window, type \"ipython notebook.\"  This will open a blank iPython notebook environment in your browser.\n",
      "* Then follow the prompt near the top to create or import an iPython Notebook file.\n",
      "<p>&nbsp;</p>\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<div id=\"recipes\" style=\"padding: 1em; background-color: #cccccc;\">WhatEvery1Says Project Task Recipes. <span style=\"font-size: 50%;\">(<a href=\"#top\">Return to Table of Contents</a>)<span></div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<div id=\"recipes-getnyt\" style=\"padding: 1em; background-color: #F1F1F1;\">C. Using the \"get-nytimes-articles.py\" script <span style=\"font-size: 50%;\">(<a href=\"#top\">Return to table of contents</a>)<span></div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "C. Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* This Python script executes a query on the NYT articles API and automatically processes/aggregates the JSON-format results in a TSV (tab separated values) file, ready for import into Excel or another spreadsheet program.  The resulting sheet has columns for all the returned results--including URL, byline, type of document, section of NYT, etc.\n",
      "* By default, the Python script is designed to submit a simple \"q\" query for a keyword or phrase.  One sets the query in the separate plain-text \"settings.cfg\" file (in the config subfolder of the script).  For example, to search for \"apples, oranges, cherries,\" you would set the query=apples+oranges+cherries.\n",
      "* In order to use the Python script to submit to the NYT a \"fq\" query and other modifications allowing for finer adjustments of the sort we will be interested in for WE1S, one needs to tweak both the query= line in the \"settings.cfg\" file and the \"request_string\" line in the main Python script (so as to generate the total request string that one wants).\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "C. Resources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* Github source for get-nytimes-articles.py script (and an example settings.cfg file) -- Use \"download zip\" at bottom of right sidebar on the Github web page to download the whole package: https://github.com/casmlab/get-nytimes-articles\n",
      "* Note 1: In the settings.cfg file or the script, you need to define a folder on your computer for storing the JSON files that a successful query to the NYT generates (e.g., json_folder = C:/workspace/).  You also need to define the path of the TSV file in which  the script aggregates the individual JSON results (e.g., tsv_file = C:/workspace/output.tsv).  In addition, you need to input your NYT API key.\n",
      "* Note 2: If you run the script and the query is valid, then your Python window will appear to freeze for some time (a really long time if you are searching through a large span of dates) before you are returned to a command prompt.  Check your defined workspace for the JSON and TSV files to see if the script is working properly.  You'll see one JSON file after another added; and the TSV file is added to in each instance.  If your query is invalid, you'll either get an \"invalid syntax\" error message from Python or a running series of \"error . . .  bad request\" messages that have to be stopped by interrupting or restarting the Python kernel.\n",
      "* Note 3: If you are just experimenting, set the begin and end date parameters of a search to a short period (e.g., three months or less).  To give you a sense of time scale: when I searched for \"humanities\" over three years of the NYT, it took about an hour.  Doing the same search for three months makes it a reasonable 5-10 minutes.  (You can check individual JSON files as they are added at any time to see what is happening if you want, even before the whole search is completed.  So you don't have to wait for a completed search.)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "C. Recipes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Use the get-nytmes-articles.py script to do a proximity search for the words \"humanities\" and \"arts\" within 3 words of each other:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. In the settings.cfg file, set the query line as follows (using Lucene query syntax):\n",
      "query = \"humanities+arts\"~3\n",
      "2. In the Python script, set the request string to use \"fq\" instead of just \"q.\"  So the line would begin:\n",
      "request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?fq="
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Use the get-nytmes-articles.py script to do a proximity search and also limit the search to \"The New York Times\" as the \"source\" field (versus all the other sources that the NYT carries, including AP, International Herald Tribune, etc.):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "1. In the settings.cfg file, set the query line as follows (using Lucene query syntax): <br />query = \"humanities+arts\"~3\n",
      "2. In Python script:<br />request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=source%3A%28%22The+New+York+Times%22%29+AND+\" + query + \"&begin_date=\" + date + \"&end_date=\" + date + \"&page=\" + str(page) + \"&api-key=\" + api_key\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Use the get-nytimes-articles.py script to do a proximity search, limit the search to \"The New York Times\" as the \"source\" field, and exclude selected \"news_desks\":"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. In the settings.cfg file, set the query line as follows (using Lucene query syntax):\n",
      "query = \"humanities+arts\"~3\n",
      "2. In the Python script, set the request string to use \"fq\" instead of just \"q.\"  So the line would begin:\n",
      "request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=source%3A%28%22The+New+York+Times%22%29+NOT+news_desk.contains%3A%28%22Sports%22+%22Classified%22%29+AND+\" + query + \"&begin_date=\" + date + \"&end_date=\" + date + \"&page=\" + str(page) + \"&api-key=\" + api_key"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<div id=\"recipes-downloading\" style=\"padding: 1em; background-color: #F1F1F1;\">D. Downloading web pages from list of URLs <span style=\"font-size: 50%;\">(<a href=\"#top\">Return to table of contents</a>)<span></div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "D. Resources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. DownloadThemAll addon for Firefox, https://addons.mozilla.org/en-US/firefox/addon/downthemall/\n",
      "2. Note: This seems the simplest solution.  Wget for various OS platforms is also used for this purpose."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<div id=\"recipes-scraping\" style=\"padding: 1em; background-color: #F1F1F1;\">E. Scraping plain text from web pages or PDFs <span style=\"font-size: 50%;\">(<a href=\"#top\">Return to table of contents</a>)<span></div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "E. Recipes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the below recipies, \"testfile,\" \"raw,\" etc. are arbitrarily named variables. For file paths, etc., substitute your own."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Read local HTML file into Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "print testfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Scrape plain text from a local Web page using Python and BeautifulSoup 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "print raw "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Scrape all plain text from an online Web page using Python and BeautifulSoup 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "import re\n",
      "\n",
      "url=\"http://www.english.ucsb.edu/\"\n",
      "page=urllib2.urlopen(url)\n",
      "soup = BeautifulSoup(page.read())\n",
      "print(soup.get_text())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4.Scrape plain text of <span style=\"color: red;\">article body</span> in NYT article using Python and BeautifulSoup 4 --\n",
      "Method: filter for &lt;p class=\"story-body-text story-content\"&gt; tags in NYT article; then loop through the tags and print only the text they contain: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "soup = BeautifulSoup(testfile)\n",
      "\n",
      "soup.find_all('p',{'class':'story-body-text story-content'})\n",
      "\n",
      "for p_tag in soup.find_all('p'):\n",
      "    print p_tag.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "5. Scrape plain text of <span style=\"color: red;\">headline + article body</span> in NYT article using Python and BeautifulSoup 4 --\n",
      "Method: filter for &lt;h1 ... class=\"story-heading\"&gt; + &lt;p class=\"story-content story-body-text\"%gt; tags in NYT article; then loop through the tags and print only the text they contain:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "soup = BeautifulSoup(testfile)\n",
      "\n",
      "soup.find_all('h1',{'class':'story-heading'})\n",
      "for h1_tag in soup.find_all('h1'):\n",
      "    print h1_tag.text\n",
      "\n",
      "soup.find_all('p',{'class':'story-body-text story-content'})\n",
      "\n",
      "for p_tag in soup.find_all('p'):\n",
      "    print p_tag.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "6. Scrape plain text of <span style=\"color: red;\">headline + article body</span> in NYT article using Python and BeautifulSoup 4 and append to a file called <span style=\"color: red;\">\"text_harvest.txt\"</span>--\n",
      "Method: filter for &lt;h1 ... class=\"story-heading\"&gt; + &lt;p class=\"story-content story-body-text\"&gt; tags in NYT article; then loop through the tags and append the text they contain to a Unicode text file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "import codecs\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/06reuters-britain-graduates.html\").read()\n",
      "\n",
      "soup = BeautifulSoup(testfile)\n",
      "\n",
      "soup.find_all('h1',{'class':'story-heading'})\n",
      "\n",
      "for h1_tag in soup.find_all('h1'):\n",
      "    with codecs.open(\"C:/workspace/text_harvest.txt\", \"a\", \"utf-8\") as alltext:\n",
      "        alltext.write(h1_tag.text)\n",
      "\n",
      "soup.find_all('p',{'class':'story-body-text story-content'})\n",
      "for p_tag in soup.find_all('p'):\n",
      "    with codecs.open(\"C:/workspace/text_harvest.txt\", \"a\", \"utf-8\") as alltext:\n",
      "        alltext.write(p_tag.text)\n",
      "    alltext.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "7. Loop through and scrape a whole folder of locally-stored HTML files previously downloaded from NYT; accumulate plain text results in a file called \"text_harvest.txt\" with divisions between articles indicated by line breaks and 10 ampersands **********"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This script\n",
      "# (1) takes as input a folder of locally-stored HTML pages previously downloaded from the NY Times;\n",
      "# (2) scrapes plain text from the article title and body of each article using BeautifulSoup 4;\n",
      "# (3) and appends the text on an article-by-article basis in a Unicode harvester file called \"text_harvest.txt\"\n",
      "# Text for individual articles are separated in the harvester file by line breaks and the string @@@@@@@@@@\n",
      "# Customize path names in the following for your computer and development environment.\n",
      "\n",
      "# import modules\n",
      "\n",
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "import codecs\n",
      "import glob\n",
      "\n",
      "# (Start of outer loop) Inputs folder of locally-stored HTML pages by\n",
      "# using glob module to walk through folder and open each article.\n",
      "# Then makes soup from each article (processes it in BeautifulSoup)\n",
      "\n",
      "path = 'C:/Users/Alan/Google Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/*.html'   \n",
      "files=glob.glob(path)   \n",
      "for file in files:\n",
      "    f=open(file, 'r')\n",
      "\n",
      "    # (Start of inner loops) Uses BeautifulSoup) to scrape plain text of article title and body;\n",
      "    #  then appends to text_harvest.txt file.\n",
      "\n",
      "    soup = BeautifulSoup(f)\n",
      "\n",
      "    # Use BeautifulSoup to find and extract NYT article title as plain text, appending to text_harvest.txt\n",
      "    # (NYT articles put titles in a h1 tag that include the attribute class=story-heading)\n",
      "    soup.find_all('h1',{'class':'story-heading'})\n",
      "    for h1_tag in soup.find_all('h1'):\n",
      "        with codecs.open(\"C:/workspace/text_harvest.txt\", \"a\", \"utf-8\") as alltext:\n",
      "            alltext.write(h1_tag.text)\n",
      "\n",
      "    # Use BeautifulSoup to find and extract NYT article body as plain text, appending to text_harvest.txt\n",
      "    # (NYT articles put the body of stories in a series of p tags, each of which includes the attribute class=story-body-text story content)\n",
      "    soup.find_all('p',{'class':'story-body-text story-content'})\n",
      "    for p_tag in soup.find_all('p'):\n",
      "        with codecs.open(\"C:/workspace/text_harvest.txt\", \"a\", \"utf-8\") as alltext:\n",
      "            alltext.write(p_tag.text)\n",
      "        alltext.close()\n",
      "\n",
      "    # Add line break and the string @@@@@@@@@@ (ten ampersands) to above scrape of an individual article to indicate end-of-article division in the text-harvester.txt file\n",
      "    with codecs.open(\"C:/workspace/text_harvest.txt\", \"a\", \"utf-8\") as alltext:\n",
      "        alltext.write(\"\\n@@@@@@@@@@\\n\")\n",
      "        \n",
      "    # (Close of inner loops)\n",
      "    \n",
      "f.close() # (Close of outer loop)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<div id=\"recipes-nltk\" style=\"padding: 1em; background-color: #F1F1F1;\">F. Using Python NLTK package (with BeautifulSoup 4) to do text manipulation and analysis <span style=\"font-size: 50%;\">(<a href=\"#top\">Return to table of contents</a>)<span></div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "F. Recipes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the below recipies, \"testfile,\" \"raw,\" etc. are arbitrarily named variables.<br /> For file paths, etc., substitute your own."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK, then tokenize and inspect tokens using NLTK"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "number_of_tokens= len(tokens)\n",
      "s = \"Total number of tokens = \"\n",
      "print s+str(number_of_tokens)\n",
      "\n",
      "first_ten_tokens = tokens[:10]\n",
      "s2 = \"First 10 tokens =\"\n",
      "print s2+str(first_ten_tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">concordance (for a target word)</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.concordance(\"humanities\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">collocates in document</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.collocations()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">words used in similar context as target word</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.similar(\"art\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "5. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">common contexts of 2 or more target words</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.common_contexts([\"business\", \"arts\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}