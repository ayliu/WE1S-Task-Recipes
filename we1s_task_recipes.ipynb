{
 "metadata": {
  "name": "",
  "signature": "sha256:9174a5e9bb7958331c65200061823b1065d3f5c8fe64774c56f8852a9df55c79"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<div style=\"padding: 1em; background-color: #cccccc;\">Starting an iPython Notebook:</div>"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* Open Canopy\n",
      "* Tools > Canopy > Canopy Command Prompt\n",
      "* Then: \"ipython notebook\" will open the iPython notebook environment\n",
      "* Then \"create new notebook\""
     ]
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "<div style=\"padding: 1em; background-color: #cccccc;\">WhatEvery1Says Project Task Recipes:</div"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<div style=\"padding: 1em; background-color: #F1F1F1;\">C. Using the \"get-nytimes-articles.py\" script</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Overview"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* This Python script executes a query on the NYT articles API and automatically processes/aggregates the JSON-format results in a TSV (tab separated values) file, ready for import into Excel or another spreadsheet program.  The resulting sheet has columns for all the returned results--including URL, byline, type of document, section of NYT, etc.\n",
      "* By default, the Python script is designed to submit a simple \"q\" query for a keyword or phrase.  One sets the query in the separate plain-text \"settings.cfg\" file (in the config subfolder of the script).  For example, to search for \"apples, oranges, cherries,\" you would set the query=apples+oranges+cherries.\n",
      "* In order to use the Python script to submit to the NYT a \"fq\" query and other modifications allowing for finer adjustments of the sort we will be interested in for WE1S, one needs to tweak both the query= line in the \"settings.cfg\" file and the \"request_string\" line in the main Python script (so as to generate the total request string that one wants).\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "* Github source for get-nytimes-articles.py script (and an example settings.cfg file) -- Use \"download zip\" at bottom of right sidebar on the Github web page to download the whole package: https://github.com/casmlab/get-nytimes-articles\n",
      "* Note 1: In the settings.cfg file or the script, you need to define a folder on your computer for storing the JSON files that a successful query to the NYT generates (e.g., json_folder = C:/workspace/).  You also need to define the path of the TSV file in which  the script aggregates the individual JSON results (e.g., tsv_file = C:/workspace/output.tsv).  In addition, you need to input your NYT API key.\n",
      "* Note 2: If you run the script and the query is valid, then your Python window will appear to freeze for some time (a really long time if you are searching through a large span of dates) before you are returned to a command prompt.  Check your defined workspace for the JSON and TSV files to see if the script is working properly.  You'll see one JSON file after another added; and the TSV file is added to in each instance.  If your query is invalid, you'll either get an \"invalid syntax\" error message from Python or a running series of \"error . . .  bad request\" messages that have to be stopped by interrupting or restarting the Python kernel.\n",
      "* Note 3: If you are just experimenting, set the begin and end date parameters of a search to a short period (e.g., three months or less).  To give you a sense of time scale: when I searched for \"humanities\" over three years of the NYT, it took about an hour.  Doing the same search for three months makes it a reasonable 5-10 minutes.  (You can check individual JSON files as they are added at any time to see what is happening if you want, even before the whole search is completed.  So you don't have to wait for a completed search.)\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Recipes"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Use the get-nytmes-articles.py script to do a proximity search for the words \"humanities\" and \"arts\" within 3 words of each other:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. In the settings.cfg file, set the query line as follows (using Lucene query syntax):\n",
      "query = \"humanities+arts\"~3\n",
      "2. In the Python script, set the request string to use \"fq\" instead of just \"q.\"  So the line would begin:\n",
      "request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?fq="
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Use the get-nytmes-articles.py script to do a proximity search and also limit the search to \"The New York Times\" as the \"source\" field (versus all the other sources that the NYT carries, including AP, International Herald Tribune, etc.):"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "1. In the settings.cfg file, set the query line as follows (using Lucene query syntax): <br />query = \"humanities+arts\"~3\n",
      "2. In Python script:<br />request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=source%3A%28%22The+New+York+Times%22%29+AND+\" + query + \"&begin_date=\" + date + \"&end_date=\" + date + \"&page=\" + str(page) + \"&api-key=\" + api_key\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Use the get-nytimes-articles.py script to do a proximity search, limit the search to \"The New York Times\" as the \"source\" field, and exclude selected \"news_desks\":"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. In the settings.cfg file, set the query line as follows (using Lucene query syntax):\n",
      "query = \"humanities+arts\"~3\n",
      "2. In the Python script, set the request string to use \"fq\" instead of just \"q.\"  So the line would begin:\n",
      "request_string = \"http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=source%3A%28%22The+New+York+Times%22%29+NOT+news_desk.contains%3A%28%22Sports%22+%22Classified%22%29+AND+\" + query + \"&begin_date=\" + date + \"&end_date=\" + date + \"&page=\" + str(page) + \"&api-key=\" + api_key"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<div style=\"padding: 1em; background-color: #F1F1F1;\">D. Downloading web pages from list of URLs</div"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Resources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "1. DownloadThemAll addon for Firefox, https://addons.mozilla.org/en-US/firefox/addon/downthemall/\n",
      "2. Note: This seems the simplest solution.  Wget for various OS platforms is also used for this purpose."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<div style=\"padding: 1em; background-color: #F1F1F1;\">E. Scraping plain text from web pages or PDFs</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Recipes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the below recipies, \"testfile,\" \"raw,\" etc. are arbitrarily named variables.<br /> For file paths, etc., substitute your own."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Read local HTML file into Python"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "print testfile"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Scrape plain text from a local Web page using Python and BeautifulSoup 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "print raw "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Scrape all plain text from an online Web page using Python and BeautifulSoup 4"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import urllib2\n",
      "import re\n",
      "\n",
      "url=\"http://www.english.ucsb.edu/\"\n",
      "page=urllib2.urlopen(url)\n",
      "soup = BeautifulSoup(page.read())\n",
      "print(soup.get_text())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4.Scrape plain text of <span style=\"color: red;\">article body</span> in NYT article using Python and BeautifulSoup 4 --\n",
      "Method: filter for &lt;p class=\"story-body-text story-content\"&gt; tags in NYT article; then loop through the tags and print only the text they contain: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "soup = BeautifulSoup(testfile)\n",
      "\n",
      "soup.find_all('p',{'class':'story-body-text story-content'})\n",
      "\n",
      "for p_tag in soup.find_all('p'):\n",
      "    print p_tag.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "5. Scrape plain text of <span style=\"color: red;\">headline + article body</span> in NYT article using Python and BeautifulSoup 4 --\n",
      "Method: filter for &lt;h1 ... class=\"story-heading\"&gt; + &lt;p class=\"story-content story-body-text\"%gt; tags in NYT article; then loop through the tags and print only the text they contain:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "soup = BeautifulSoup(testfile)\n",
      "\n",
      "soup.find_all('h1',{'class':'story-heading'})\n",
      "for h1_tag in soup.find_all('h1'):\n",
      "    print h1_tag.text\n",
      "\n",
      "soup.find_all('p',{'class':'story-body-text story-content'})\n",
      "\n",
      "for p_tag in soup.find_all('p'):\n",
      "    print p_tag.text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "6. Scrape plain text of <span style=\"color: red;\">headline + article body</span> in NYT article using Python and BeautifulSoup 4 and append to a file called <span style=\"color: red;\">\"text_harvest.txt\"</span>--\n",
      "Method: filter for &lt;h1 ... class=\"story-heading\"&gt; + &lt;p class=\"story-content story-body-text\"&gt; tags in NYT article; then loop through the tags and append the text they contain to a Unicode text file:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "from bs4 import BeautifulSoup\n",
      "import codecs\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/06reuters-britain-graduates.html\").read()\n",
      "\n",
      "soup = BeautifulSoup(testfile)\n",
      "\n",
      "soup.find_all('h1',{'class':'story-heading'})\n",
      "\n",
      "for h1_tag in soup.find_all('h1'):\n",
      "    with codecs.open(\"C:/workspace/text_harvest.txt\", \"a\", \"utf-8\") as alltext:\n",
      "        alltext.write(h1_tag.text)\n",
      "\n",
      "soup.find_all('p',{'class':'story-body-text story-content'})\n",
      "for p_tag in soup.find_all('p'):\n",
      "    with codecs.open(\"C:/workspace/text_harvest.txt\", \"a\", \"utf-8\") as alltext:\n",
      "        alltext.write(p_tag.text)\n",
      "    alltext.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "<div style=\"padding: 1em; background-color: #F1F1F1;\">F. Using Python NLTK package (with BeautifulSoup 4) to do text manipulation and analysis</div>"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Recipes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the below recipies, \"testfile,\" \"raw,\" etc. are arbitrarily named variables.<br /> For file paths, etc., substitute your own."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "1. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK, then tokenize and inspect tokens using NLTK"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "number_of_tokens= len(tokens)\n",
      "s = \"Total number of tokens = \"\n",
      "print s+str(number_of_tokens)\n",
      "\n",
      "first_ten_tokens = tokens[:10]\n",
      "s2 = \"First 10 tokens =\"\n",
      "print s2+str(first_ten_tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "2. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">concordance (for a target word)</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.concordance(\"humanities\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "3. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">collocates in document</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.collocations()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "4. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">words used in similar context as target word</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.similar(\"art\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "5. Scrape plain text from a local Web page using Python, BeautifulSoup 4, and NLTK; tokenize; then find the following using NLTK-- <br /><span style=\"color:red\">common contexts of 2 or more target words</span>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib2\n",
      "import nltk, re, pprint\n",
      "from nltk import word_tokenize\n",
      "from bs4 import BeautifulSoup\n",
      "\n",
      "testfile = urllib2.urlopen(\"file:///C:/Users/Alan/Google%20Drive/4Humanities/we1s/algorithmic/experiments/alan/nytimes/2013-2014-articles-the-humanities/downloads/08reuters-usa-people-mondale.html\").read()\n",
      "\n",
      "raw = BeautifulSoup(testfile).get_text()\n",
      "\n",
      "tokens = word_tokenize(raw)\n",
      "\n",
      "text = nltk.Text(tokens)\n",
      "\n",
      "text.common_contexts([\"business\", \"arts\"])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}